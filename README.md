## Awesome-DL-Inference
This is a paper list about DL inference.

### DNN Inference
[NSDI'17][UCB] [Clipper: A Low-Latency Online Prediction Serving System](https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/crankshaw)  
[SOSP'19][Amason] [Nexus: a GPU cluster engine for accelerating DNN-based video analysis](https://dl.acm.org/doi/abs/10.1145/3341301.3359658)  
[ATC'19][HKUST] [MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving](https://www.usenix.org/conference/atc19/presentation/zhang-chengliang)  
[SoCC'20][UCR] [GSLICE: Controlled Spatial Sharing of GPUs for a Scalable Inference Platform](https://dl.acm.org/doi/abs/10.1145/3419111.3421284)  
[ATC'22][KAIST] [Serving Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing](https://www.usenix.org/conference/atc22/presentation/choi-seungbeom)  
[ATC'21][Stanford] [INFaaS: Automated Model-less Inference Serving](https://www.usenix.org/conference/atc21/presentation/romero)  

[OSDI'18][SNU] [PRETZEL: Opening the Black Box of Machine Learning Prediction Serving Systems](https://www.usenix.org/conference/osdi18/presentation/lee)  
[OSDI'20][MPI] [Serving DNNs like Clockwork: Performance Predictability from the Bottom Up](https://www.usenix.org/conference/osdi20/presentation/gujarati)  
[OSDI'22][SJTU] [Microsecond-scale Preemption for Concurrent GPU-accelerated DNN Inferences](https://www.usenix.org/conference/osdi22/presentation/han)  
[OSDI'23][SJTU] [Optimizing Dynamic Neural Networks with Brainstorm](https://www.usenix.org/conference/osdi23/presentation/cui)  
[SOSP'19][CMU] [Parity models: erasure-coded resilience for prediction serving systems](https://dl.acm.org/doi/10.1145/3341301.3359654)  
[SOSP'23][UPenn] [Paella: Low-latency Model Serving with Software-defined GPU Scheduling](https://dl.acm.org/doi/10.1145/3600006.3613163)  
[NSDI'21][Yale] [Mistify: Automating DNN Model Porting for On-Device Inference at the Edge](https://www.usenix.org/conference/nsdi21/presentation/guo)  
[NSDI'21][UCB] [On the Use of ML for Blackbox System Performance Prediction](https://www.usenix.org/conference/nsdi21/presentation/fu)  
[NSDI'22][UPenn] [Cocktail: A Multidimensional Optimization for Model Serving in Cloud](https://www.usenix.org/conference/nsdi22/presentation/gunasekaran)  
[NSDI'23][UWaterloo] [SHEPHERD: Serving DNNs in the Wild](https://www.usenix.org/conference/nsdi23/presentation/zhang-hong)  
[SIGCOMM'23][MIT] [Lightning: A Reconfigurable Photonic-Electronic SmartNIC for Fast and Energy-Efficient Inference](https://dl.acm.org/doi/10.1145/3603269.3604821)  
[SIGCOMM'23][UVA] [AdaInf: Data Drift Adaptive Scheduling for Accurate and SLO-guaranteed Multiple-Model Inference Serving at Edge Servers](https://dl.acm.org/doi/10.1145/3603269.3604830)  
[ATC'18][CMU] [Cavs: An Efficient Runtime System for Dynamic Neural Networks](https://www.usenix.org/conference/atc18/presentation/xu-shizen)  
[ATC'18][MSR] [DeepCPU: Serving RNN-based Deep Learning Models 10x Faster](https://www.usenix.org/conference/atc18/presentation/zhang-minjia)  
[ATC'19][Amazon] [Optimizing CNN Model Inference on CPUs](https://www.usenix.org/conference/atc19/presentation/liu-yizhi)  
[ATC'22][SJTU] [DVABatch: Diversity-aware Multi-Entry Multi-Exit Batching for Efficient Processing of DNN Services on GPUs](https://www.usenix.org/conference/atc22/presentation/cui)  
[ATC'22][PKU] [PetS: A Unified Framework for Parameter-Efficient Transformers Serving](https://www.usenix.org/conference/atc22/presentation/zhou-zhe)  
[ATC'22][TJU] [Tetris: Memory-efficient Serverless Inference through Tensor Sharing](https://www.usenix.org/conference/atc22/presentation/li-jie)  
[ATC'22][HKU] [SOTER: Guarding Black-box Inference for General Neural Networks at the Edge](https://www.usenix.org/conference/atc22/presentation/shen)  



### LLM Inference
[OSDI'22][SNU] [ORCA: A Distributed Serving System for Transformer-Based Generative Models](https://www.usenix.org/conference/osdi22/presentation/yu)   
[arXiv'23][PKU] [Fast Distributed Inference Serving for Large Language Models](https://arxiv.org/abs/2305.05920)    
[SOSP'23][UCB] [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://dl.acm.org/doi/abs/10.1145/3600006.3613165) 

